"use strict";(globalThis.webpackChunkutk09_github_io=globalThis.webpackChunkutk09_github_io||[]).push([[5133],{1644(e,t,n){n.d(t,{A:()=>s});const s=n.p+"assets/images/each-quote-40c7c6ea1e5a7c291e1204b66b84b778.png"},1799(e,t,n){n.d(t,{A:()=>s});const s=n.p+"assets/images/div-class-quote-75d9f6f2e82a65c36a016857f9f74720.png"},5843(e,t,n){n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"data-science/2024-01-02-data-science-part-1-getting-the-data","title":"Data Science Part 1 - Getting the Data","description":"In this part, we will be discussing how to get the data for our data science project.","source":"@site/tutorials/data-science/2024-01-02-data-science-part-1-getting-the-data.md","sourceDirName":"data-science","slug":"/data-science/data-science-part-1-getting-the-data","permalink":"/tutorials/data-science/data-science-part-1-getting-the-data","draft":false,"unlisted":false,"editUrl":"https://github.com/utk09/utk09.github.io/edit/main/tutorials/data-science/2024-01-02-data-science-part-1-getting-the-data.md","tags":[{"inline":true,"label":"python","permalink":"/tutorials/tags/python"},{"inline":true,"label":"data-science","permalink":"/tutorials/tags/data-science"},{"inline":true,"label":"data-collection","permalink":"/tutorials/tags/data-collection"},{"inline":true,"label":"data-cleaning","permalink":"/tutorials/tags/data-cleaning"},{"inline":true,"label":"web-scraping","permalink":"/tutorials/tags/web-scraping"}],"version":"current","frontMatter":{"slug":"data-science-part-1-getting-the-data","title":"Data Science Part 1 - Getting the Data","description":"In this part, we will be discussing how to get the data for our data science project.","authors":["utk09"],"tags":["python","data-science","data-collection","data-cleaning","web-scraping"]},"sidebar":"tutorialSidebar","previous":{"title":"Tutorials","permalink":"/tutorials/"}}');var a=n(4848),i=n(8453);const r={slug:"data-science-part-1-getting-the-data",title:"Data Science Part 1 - Getting the Data",description:"In this part, we will be discussing how to get the data for our data science project.",authors:["utk09"],tags:["python","data-science","data-collection","data-cleaning","web-scraping"]},o=void 0,c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Libraries",id:"libraries",level:2},{value:"Scraping Data",id:"scraping-data",level:2},{value:"Setting Up Your Environment",id:"setting-up-your-environment",level:3},{value:"Scrape the Data",id:"scrape-the-data",level:3},{value:"Code",id:"code",level:3},{value:"Running the Script",id:"running-the-script",level:3},{value:"Output",id:"output",level:3},{value:"Conclusion",id:"conclusion",level:2}];function h(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"In this series, we'll explore the steps in a data science project, starting with data acquisition."}),"\n",(0,a.jsx)(t.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(t.p,{children:"Data is essential for any data science project. It underpins analysis and model building. Common data acquisition methods include:"}),"\n",(0,a.jsxs)(t.ol,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Web Scraping"}),": This is the process of extracting data from websites. We can use libraries like ",(0,a.jsx)(t.code,{children:"requests"})," and ",(0,a.jsx)(t.code,{children:"beautifulsoup"})," in Python to scrape data from websites."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"APIs"}),": Many websites provide APIs to access their data. We can use these APIs to get the data we need."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Databases"}),": We can also get data from databases. There are many public databases available on the internet that we can use for our projects."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"We'll focus on web scraping first."}),"\n",(0,a.jsx)(t.h2,{id:"libraries",children:"Libraries"}),"\n",(0,a.jsx)(t.p,{children:"Web scraping involves extracting data from websites. Python offers several libraries for this:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://docs.python-requests.org/en/master/",children:"requests"}),": A simple library for making HTTP requests."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://www.python-httpx.org/",children:"httpx"}),": A library for making HTTP requests."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://urllib3.readthedocs.io/en/latest/",children:"urllib3"}),": A powerful library for making HTTP requests."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://beautiful-soup-4.readthedocs.io/en/latest/",children:"beautifulsoup"}),": Parses HTML to retrieve necessary data."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://scrapy.org/",children:"scrapy"}),": A comprehensive tool offering extensive web scraping capabilities."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://selenium-python.readthedocs.io/",children:"selenuim"}),": Automates web browsers for data extraction."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://playwright.dev/python/docs/intro",children:"playwright"}),": A newer option offering a straightforward approach to web scraping."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://mechanicalsoup.readthedocs.io/en/stable/index.html",children:"MechanicalSoup"}),": Automates web browser interactions for data retrieval."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.a,{href:"https://lxml.de/",children:"lxml"}),": A library for processing XML and HTML documents."]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"scraping-data",children:"Scraping Data"}),"\n",(0,a.jsxs)(t.p,{children:["In this section, we'll learn how to extract data from ",(0,a.jsx)(t.a,{href:"https://quotes.toscrape.com/",children:(0,a.jsx)(t.strong,{children:"quotes.toscrape.com"})})," using Python's ",(0,a.jsx)(t.code,{children:"requests"})," and ",(0,a.jsx)(t.code,{children:"BeautifulSoup"})," libraries."]}),"\n",(0,a.jsx)(t.h3,{id:"setting-up-your-environment",children:"Setting Up Your Environment"}),"\n",(0,a.jsxs)(t.p,{children:["First, ensure you have ",(0,a.jsx)(t.a,{href:"https://wiki.python.org/moin/BeginnersGuide/Download",children:(0,a.jsx)(t.strong,{children:"Python"})})," installed. Then, set up a virtual environment using ",(0,a.jsx)(t.a,{href:"https://docs.python.org/3/library/venv.html",children:(0,a.jsx)(t.strong,{children:"venv"})})," ",(0,a.jsx)(t.em,{children:"(which is included in Python 3.3 and later)"})," to install the required libraries."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"# Create a virtual environment\n$> python -m venv data_science_env\n\n# Activate the virtual environment\n\n# On Windows, use:\n$> data_science_env\\Scripts\\activate\n\n# On Unix or MacOS, use:\n$> source data_science_env/bin/activate\n\n# Install required libraries\n$> pip install requests beautifulsoup4\n"})}),"\n",(0,a.jsxs)(t.p,{children:["If you prefer not to install anything on your local machine, ",(0,a.jsx)(t.a,{href:"https://replit.com/",children:(0,a.jsx)(t.strong,{children:"Replit"})})," is a convenient alternative. It's an online IDE that supports Python without needing any local setup."]}),"\n",(0,a.jsx)(t.h3,{id:"scrape-the-data",children:"Scrape the Data"}),"\n",(0,a.jsx)(t.p,{children:"Now, let's proceed to the actual scraping. We aim to collect quotes, authors, and tags from the site and save them into a CSV file for further analysis or use."}),"\n",(0,a.jsx)(t.p,{children:"Here\u2019s a simplified explanation of the process:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Request Website Data"}),": We use ",(0,a.jsx)(t.code,{children:"requests"})," to download the page content."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Parse HTML Content"}),": With ",(0,a.jsx)(t.code,{children:"BeautifulSoup"}),", we sift through the HTML to find the data we're interested in."]}),"\n",(0,a.jsxs)(t.li,{children:[(0,a.jsx)(t.strong,{children:"Extract and Save Data"}),": We pull out the required information and write it to a CSV file."]}),"\n"]}),"\n",(0,a.jsxs)(t.p,{children:["This method offers a practical approach to gather data from the web efficiently and ethically. Ensure to respect the website's ",(0,a.jsx)(t.code,{children:"robots.txt"})," rules and terms of service when scraping. In our case, the website we're scraping is designed for practice and learning purposes."]}),"\n",(0,a.jsx)(t.h3,{id:"code",children:"Code"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:'# Filename: quotes_scraper.py\n\n# Import necessary libraries\nimport requests  # Used for making HTTP requests.\nfrom bs4 import BeautifulSoup  # Used for parsing HTML content.\nimport csv  # Used for handling CSV file operations.\nimport time  # Used for introducing delays during execution.\n\n# The base URL for the quotes to scrape.\nbase_url = \'https://quotes.toscrape.com\'\n\n# Flag to check if any quotes have been found across pages.\nquotes_found = False\n\n# CSV file name to store the scraped data.\ncsv_file = "quotes.csv"\n\n# Open a new CSV file to store scraped data, ensuring it uses UTF-8 encoding.\nwith open(csv_file, "w", newline=\'\', encoding=\'utf-8\') as file:\n    writer = csv.writer(file)  # Create a CSV writer object.\n    writer.writerow(["Quote", "Author", "Tags"])  # Write the header row for the CSV file.\n\n# Loop through pages 1 to 10 of the website.\n# From https://quotes.toscrape.com/page/1/ ... https://quotes.toscrape.com/page/10/\nfor page_num in range(1, 11):\n    # Construct the URL for the current page by appending the page number.\n    url = f"{base_url}/page/{page_num}/"\n    # Make a GET request to the constructed URL.\n    response = requests.get(url)\n    # Parse the HTML content of the page using BeautifulSoup.\n    soup = BeautifulSoup(response.text, \'html.parser\')\n\n    """\n      As seen in the HTML structure of the website (ref Image 1),\n      each quote is contained within a <div class="quote"...> element.\n    """\n\n    # Find all elements with the class "quote", which contain the desired information.\n    quoteAuthorTags = soup.find_all(class_="quote")\n    # The find_all method returns a list of all matching elements.\n\n    # Check if the list of found quote elements is empty.\n    if not quoteAuthorTags:\n        # Print a message indicating no quotes were found on this page.\n        print(f"No quotes found on page {page_num}.")\n    else:\n        # If quotes are found, set the flag to True.\n        quotes_found = True\n\n        """\n          As seen in the HTML structure of the website (ref Image 2),\n          each quote block contains the <span class="text"...>,\n          <small class="author"...>, and <a class="tag"...> elements.\n          We extract the quote text, author, and tags from each quote block.\n        """\n\n        # Iterate over each quote block found on the current page.\n        for eachQuoteAuthorTag in quoteAuthorTags:\n            # Extract and clean the quote text.\n            quote = eachQuoteAuthorTag.find(class_="text").get_text(strip=True).replace(\'\u201c\', \'\').replace(\'\u201d\', \'\')\n            # (strip=True) removes leading and trailing whitespaces\n            # (replace(\'\u201c\', \'\').replace(\'\u201d\', \'\')) removes the opening and closing quotes\n\n            # Extract the author\'s name, stripping leading and trailing whitespace.\n            author = eachQuoteAuthorTag.find(class_="author").get_text(strip=True)\n            # Extract all tags associated with the quote and join them as a single string.\n            tags = \', \'.join([tag.get_text(strip=True) for tag in eachQuoteAuthorTag.find_all(\'a\', class_=\'tag\')])\n\n            # Append the extracted data to the CSV file.\n            with open(csv_file, "a", newline=\'\', encoding=\'utf-8\') as file:\n                writer = csv.writer(file)  # Create a CSV writer object.\n                writer.writerow([quote, author, tags])  # Write the extracted data as a row in the CSV file.\n\n        # Print a message indicating successful scraping of the current page.\n        print(f"Page {page_num} scraped successfully.")\n    # Pause the loop for 2 seconds to avoid overwhelming the server.\n    time.sleep(2)\n\n# After finishing all pages, check if any quotes were found.\nif not quotes_found:\n    # If no quotes were found, print a message indicating that.\n    print("No quotes were found on any of the pages.")\nelse:\n    # If quotes were found, print a completion message.\n    print("Scraping completed successfully.")\n\n# Check if quotes were found before trying to read the file.\nif quotes_found:\n    # Print a message indicating that the script will display the first 5 rows from csv_file.\n    print(f"Displaying the first 5 rows from {csv_file}:")\n    # Open the csv_file file in read mode.\n    with open(csv_file, \'r\', encoding=\'utf-8\') as file:\n        reader = csv.reader(file)  # Create a CSV reader object.\n        # Iterate through the first 5 rows of the CSV file.\n        for i, row in enumerate(reader):\n            if i >= 5: break  # Stop after printing 5 lines.\n            print(row)  # Print each row.\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Image 1: Each quote contains a ",(0,a.jsx)(t.code,{children:"div"})," with the class ",(0,a.jsx)(t.code,{children:"quote"})]}),"\n",(0,a.jsx)(t.table,{children:(0,a.jsx)(t.thead,{children:(0,a.jsx)(t.tr,{children:(0,a.jsx)(t.th,{children:(0,a.jsx)(t.img,{alt:"Image 1",src:n(1799).A+"",title:"Each quote div",width:"2734",height:"642"})})})})}),"\n",(0,a.jsxs)(t.p,{children:["Image 2: Each quote block contains the ",(0,a.jsx)(t.code,{children:"text"}),", ",(0,a.jsx)(t.code,{children:"author"}),", and ",(0,a.jsx)(t.code,{children:"tag"})]}),"\n",(0,a.jsx)(t.table,{children:(0,a.jsx)(t.thead,{children:(0,a.jsx)(t.tr,{children:(0,a.jsx)(t.th,{children:(0,a.jsx)(t.img,{alt:"Image 2",src:n(1644).A+"",title:"Each quote block contains the text, author, and tag",width:"3000",height:"1002"})})})})}),"\n",(0,a.jsx)(t.h3,{id:"running-the-script",children:"Running the Script"}),"\n",(0,a.jsxs)(t.p,{children:["To run the script, save it to a file (e.g., ",(0,a.jsx)(t.code,{children:"quotes_scraper.py"}),") and execute it using the command:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-bash",children:"$> python quotes_scraper.py\n"})}),"\n",(0,a.jsxs)(t.p,{children:["On Replit, you can simply click the ",(0,a.jsx)(t.strong,{children:'"Run"'})," button if you've added this code in ",(0,a.jsx)(t.code,{children:"main.py"})]}),"\n",(0,a.jsx)(t.h3,{id:"output",children:"Output"}),"\n",(0,a.jsx)(t.p,{children:"After running the script, you should see a message indicating that the scraping was successful. The script will also display the first 5 rows from the CSV file."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-plaintext",children:"Page 1 scraped successfully.\nPage 2 scraped successfully.\n... (output for pages 3 to 9)\nPage 10 scraped successfully.\nScraping completed successfully.\nDisplaying the first 5 rows from quotes.csv:\n['Quote', 'Author', 'Tags']\n['The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.', 'Albert Einstein', 'change, deep-thoughts, thinking, world']\n['It is our choices, Harry, that show what we truly are, far more than our abilities.', 'J.K. Rowling', 'abilities, choices']\n['There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.', 'Albert Einstein', 'inspirational, life, live, miracle, miracles']\n['The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.', 'Jane Austen', 'aliteracy, books, classic, humor']\n"})}),"\n",(0,a.jsx)(t.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(t.p,{children:"We've covered the basics of web scraping with Python and discussed ethical considerations. Next, we'll look into how to analyse the data we've gathered."})]})}function d(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},8453(e,t,n){n.d(t,{R:()=>r,x:()=>o});var s=n(6540);const a={},i=s.createContext(a);function r(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(i.Provider,{value:t},e.children)}}}]);